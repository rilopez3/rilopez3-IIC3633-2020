<h2>Comentarios sobre “Performance of Recommender Algorithms on Top-N Recommendation Tasks"</h2>

<p>En este artículo, del año 2010, Cremonesi, Koren y Turrin nos presentan un análisis comparativo y crítico sobre los algoritmos de recomendación utilizados para escenarios del tipo Top-N, casos en el que el sistema debe entregar una lista de elementos sugeridos al usuario, los autores inician su análisis cuestionando los métodos que se han utilizado para evaluar el éxito de los algoritmos. Destacan que muchos algoritmos utilizan métricas enfocadas en el error, por sobre métricas enfocadas en la precisión de las recomendaciones, caso en el cual algoritmos sofisticados, como AsySVD (descrito como un modelo del estado del arte), logra resultados similares a métodos no personalizados, lo cual da señales que no están logrando su cometido.</p>
<p>Para realizar el análisis los autores proponen un framework de comparación transversal, con el cual logran que algoritmos con distintos approach sean comparados. Una parte importante del método de comparación que proponen radica en la separación de los dataset de entrenamiento y pruebas, para los cuales se aseguran que el set de prueba contenga solo contenidos que los usuarios han evaluado con la máxima puntuación, esta separación permite crear un escenario en el cual se espera que los modelos recomienden estos contenidos, que se sabe son agradables al usuario, en un conjunto de elementos que no lo son. De esta manera, a los algoritmos se les pide que logren recomendar estos contenidos deseables en el mejor lugar posible de la lista de tamaño N de recomendación.</p>
<p>Sobre esta base de comparación, se utilizan dos métricas para evaluar el desempeño, las métricas utilizadas son recall(N) y precisión(N), utilizar estos indicadores es un gran acierto dado que permite aislar la forma en que actúa cada modelo, y se les evalúa sobre los resultados que realmente espera el usuario, que la lista de recomendados esté poblada principalmente por contenido deseable. En la lista de algoritmos a comparar se incluyen modelos no personalizados, modelos colaborativos y modelos de factores latentes, grupo en el cual presentan su algoritmo, PureSVD, como uno con enfoque directo en la precisión por sobre los ratings y que en las pruebas logra muy buenos resultados.</p>
<p>Las pruebas fueron realizadas sobre el dataset de Netflix y el dataset de Movielens, en los resultados claramente se aprecia que los algoritmos con un enfoque en precisión (PureSVD), tienen mejores resultados a aquellos con un enfoque en el error (SVD++), los autores en su análisis explican este fenómeno por la forma como actúan, en la cual los algoritmos con un enfoque en el error se enfocan en los datos disponibles (los ratings entregados por los usuarios), en contraste a los modelos con enfoque en la precisión que realizan inferencias sobre los datos faltantes.</p>
<p>Una crítica común que se suele realizar sobre los sistemas de recomendación del tipo Top-N es la alta presencia de contenido popular, los autores se suman a esta crítica e incluyen en su análisis la comparación de los algoritmos excluyendo el contenido popular, para esto excluyen los contenidos que denominan “short-head”, que representa el 32% de las evaluaciones explicitas que realizan los usuarios, las cuales solo representan el 1.7%  y 5.5% del contenido total de Netflix y Movielens correspondientemente. Se aplica el mismo framework de comparación considerando solo los contenidos no-populares “long-tail”, el resultado es que los algoritmos no personalizados bajan drásticamente su precisión, lo cual es esperable, los algoritmos con enfoque en el error reducen su precisión, y los algoritmos con enfoque en precisión, en particular el modelo PureSVD mejora la calidad de sus recomendaciones.</p>
<p>El articulo concluye explicando las ventajas del modelo propuesto PureSVD, y los fundamentos con los cuales logro los mejores resultados, describiendo el cómo se pueden utilizar algunas de sus características en otros modelos para mejorar sus recomendaciones ante el problema Top-N.</p>
<p>El principal aporte del artículo, lo podemos relacionar a la critica y cuestionamiento sobre el uso de enfoques de error (RMSE, MAE) que no asegura una correlación directa a una mejor precisión, y que en escenarios donde se excluyen contenidos populares, principales impulsores de las métricas de optimización (son los contenidos con mayor cantidad de ratings), no tienen buenos resultados. El modelo propuesto PureSVD resulta prometedor, pero es en cierta medida opacado por una descripción demasiado genérica (los principales beneficios descritos son características inherentes a los modelos con base SVD), claramente se pudo haber realizado de manera más detalla y de mejor forma.</p>  

<h4>Rodrigo López A.</h4>
